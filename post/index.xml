<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Radiance的博客</title>
        <link>https://radiance0718.github.io/post/</link>
        <description>Recent content in Posts on Radiance的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-Hans</language>
        <lastBuildDate>Sun, 10 Mar 2024 17:36:08 +0800</lastBuildDate><atom:link href="https://radiance0718.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>最大似然估计</title>
        <link>https://radiance0718.github.io/p/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link>
        <pubDate>Sun, 10 Mar 2024 17:36:08 +0800</pubDate>
        
        <guid>https://radiance0718.github.io/p/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid>
        <description>&lt;h1 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h1&gt;
&lt;p&gt;一个带三的蒟蒻在学习ddpm，但是他发现自己已经看不懂公式了，数理统计的知识已经完全还给老师了。于是这个蒟蒻打算重新写一个博客来梳理一下&lt;/p&gt;
&lt;h2 id=&#34;基础概念什么是似然&#34;&gt;基础概念：什么是似然&lt;/h2&gt;
&lt;p&gt;我觉着似然这个名字取的非常妙，似然似然，好似+这样，好似这样。众所周知，我们在做实验时，得到的都是实验数据，而不是真实数据。而似然估计，是指根据实验得出概率估计真实概率/参数分布的过程。&lt;/p&gt;
&lt;p&gt;比如，现在市面上有一款沙卵二游，他声称自己非常良心，每次十连都有1/2的概率获得最高稀有度的角色(碧X档案的三星，fXo的5星，明X方舟的6星)。虽然舍友已经极力劝阻你不要玩这个以制作人辣鸡闻名的游戏，但是你这个冤大头还是义无反顾的往里面冲了100个648.&lt;/p&gt;
&lt;p&gt;然后你在池子里投了100次10连，你就出了2个6星&lt;del&gt;试试就逝世&lt;/del&gt;。&lt;/p&gt;
&lt;p&gt;你在暴怒之余，也在思考：真实被设置的出货概率是多少？这个时候，最大似然估计就派上用场了。&lt;/p&gt;
&lt;p&gt;根据小学的知识内容，想求什么，就把什么设为未知数。因此，我们把真实的出货概率设为$\theta$。我们可以得到，你100次10连就出货两次的概率为
&lt;/p&gt;
$$
P(出货两次) = C_{100}^2*\theta^2*(1-\theta)^{98}
$$
&lt;p&gt;
又或者，我们可以把这个公式写成似然的标准形式
&lt;/p&gt;
$$
L(\theta) = f(x=x_1|\theta)， x为出货次数，x_1 = 2
$$
&lt;p&gt;
其中$L(\theta)$就是我们的&lt;strong&gt;似然函数&lt;/strong&gt;,而右侧的$f(x=x_1|\theta)$就是“假设事件发生的概率为$\theta$，那么得到这个实验结果的概率是多大”&lt;/p&gt;
&lt;p&gt;可以很明显的从这个函数的定义看出，当这个函数取最大值的时候，$\theta$的值有最大的可能是真实值。在上述例子中，这个值是0.02（100次里出货两次，那不就是最可能2%的概率）。&lt;/p&gt;
&lt;h2 id=&#34;多次实验的似然&#34;&gt;多次实验的似然&lt;/h2&gt;
&lt;p&gt;显然100次这个样本量并不够大，为了获得足够的样本容量，你又义无反顾的向沙卵二游中投入了900个648，进行了新的9组实验，每次抽100次10连，加上最初的100次，得到的结果分别是出货{2，2，0，2，2，1，2，3，2，0}次，那么现在吗，你的似然函数长什么样呢？&lt;/p&gt;
&lt;p&gt;类似单次实验的结果，我们首先写出针对这个问题的公式。由于每次实验之间互不影响，我们可以简单的把每次实验的概率乘起来
&lt;/p&gt;
$$
P = \prod_{i = 1}^{10}(C_{100}^{x_i}*\theta^{x_i}*(1-\theta)^{100-x_i}), \\ 其中x = \{2，2，0，2，2，1，2，3，2，0\}
$$
&lt;p&gt;
接着我们把这个公式写的更加抽象化一些
&lt;/p&gt;
$$
L(\theta) = \prod_i f(x_i|\theta)
$$
&lt;p&gt;
右侧式子的意义就是“当真实参数值为$\theta$时，出现每个实验结果的概率乘积”。这就是多次实验的似然函数了，但现在的$x_i$是离散的，那么回想一下其他情况由离散改连续时，我们时如何操作的呢？——没错，积分！
&lt;/p&gt;
$$
  L(\theta) = \int f(x|\theta)dx
$$
&lt;h2 id=&#34;最大似然估计-1&#34;&gt;最大似然估计&lt;/h2&gt;
&lt;p&gt;有了似然函数，我们就想要算出其最大值取值点，而对应的$\theta$就是我们想要求出的最大似然值了。
一般来说，原函数的各种参数通常飞翔在一坨指数中，直接求原函数的极值点难度过大。考虑到概率基本是大于0的，而log函数是一个单调函数，我们可以对其取log从而让那坨混在一起的参数分离开来，同时还不改变极值点取值位置。
&lt;/p&gt;
$$
\hat\theta = argmax\ log(L(\theta))
$$
&lt;h2 id=&#34;ddpm论文中相关的部分&#34;&gt;DDPM论文中相关的部分&lt;/h2&gt;
&lt;p&gt;理解了上述部分，那么恭喜你已经理解了DDPM优化目标部分&amp;hellip;&amp;hellip;的1%不到了&lt;/p&gt;
&lt;p&gt;实际上，这一部分对于你去构筑的DDPM的代码没有任何关系，&lt;del&gt;那么我就不写了&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;由于作者贫瘠的数学知识，我也只能简单谈一下自己的理解&lt;/p&gt;
&lt;p&gt;这一部分我个人的理解是，作者并不是灵机一动想出了“哦，我们要预测噪声”,而是由最大似然估计推导过来&lt;/p&gt;
&lt;p&gt;DDPM中的$\theta$即是待优化的网络参数，而x即使在不同时刻t下加噪后的图像。我们的终极问题是，如何根据$x_0$的值去优化网络参数。作者先根据$x_0$的后验公式将其写成了有关$x_{t = 1-T}$的形式，将其转化为变分下界问题，然后运用高超的数学技巧证明了我们只需要预测噪声即可。&lt;/p&gt;
&lt;p&gt;实际上不理解这一部分也并不会对你使用DDPM造成什么困扰。正如蒟蒻作者完全看不懂公式推导但还是可以把&lt;del&gt;从实验室同学那里偷来的&lt;/del&gt;代码跑起来一样。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>DDPM</title>
        <link>https://radiance0718.github.io/p/ddpm/</link>
        <pubDate>Sun, 10 Mar 2024 14:19:05 +0800</pubDate>
        
        <guid>https://radiance0718.github.io/p/ddpm/</guid>
        <description>&lt;h1 id=&#34;ddpm学习笔记&#34;&gt;DDPM学习笔记&lt;/h1&gt;
&lt;p&gt;DDPM(Denoising Diffusion Probabilistic Models)，是在2020年Jonathan Ho，Ajay Jain，Pieter Abbeel三位作者发表的论文&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2006.11239&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;中提出的由随机噪声生成图片的模型，效果非常良好。但论文中有大量的概统内容，需要一定的数学基础。本文旨在与梳理复杂的公式，提取出其中了解代码所必需的内容&lt;/p&gt;
&lt;h2 id=&#34;框架一览&#34;&gt;框架一览&lt;/h2&gt;
&lt;p&gt;DDPM大体可以分为两部分，加噪和去噪部分。其中加噪过程对应着训练过程，去噪过程对应着生成图片的过程。&lt;/p&gt;
&lt;p&gt;加噪过程主体由两个模块组成：对图像进行不断加噪的部分和用加噪得到的结果进行训练的网络部分&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://radiance0718.github.io/DDPM/DDPM-train.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;DDPM-train&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从上图可知，需要在模块之外进行传递的主要参数有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入加噪模块的原图$x_0$&lt;/li&gt;
&lt;li&gt;从加噪模块输出，同时作为网络输入的$x_t$和时间标记t&lt;/li&gt;
&lt;li&gt;从加噪模块输出，同时作为计算loss使用的每个时刻的噪声$z_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而去噪部分则是将一张噪声图片投入到网络中，和预测的噪声多次做减法，得到原图的过程&lt;/p&gt;
&lt;h2 id=&#34;前向过程&#34;&gt;前向过程&lt;/h2&gt;
&lt;h3 id=&#34;数学推导&#34;&gt;数学推导&lt;/h3&gt;
&lt;p&gt;由于&lt;del&gt;作者太菜&lt;/del&gt;本文主旨是辅助读者梳理为了手搓ddpm你需要了解的公式，因此这里我不会列出所有的推导公式&lt;/p&gt;
&lt;p&gt;我们的目标是给$x_0$加上随机生成的噪声总共T次，每次加的噪声记为$z_t$。虽然是如此描述的，但我们加噪过程并不是简单的$x_{t+1}=x_t+z_t$。我们的模型是扩散（Diffusion）模型，因此，我们需要一个“扩散”的过程。即，我们用一个参数去控制每次加噪时噪声的占比，使得噪声是渐渐在原图上&amp;quot;浮现&amp;quot;出来。实际上我们单次加噪的模型如下：&lt;/p&gt;
$$
x_t = \sqrt{1-\beta_{t}}x_{t-1}+\sqrt{\beta_t}z_t, z_t\sim{N(0, I)}
$$
&lt;p&gt;其中$\beta$即是上文中曾提到过的用来控制加噪时噪声占比的参数。同理，$x_{t-1}$也可以写成关于$x_{t-2}, z_{t-1}, \beta_{t-1}$的形式。
&lt;/p&gt;
$$
x_{t-1} = \sqrt{1-\beta_{t-1}}x_{t-2}+\sqrt{\beta_{t-1}}z_{t-1}, z_{t-1}\sim{N(0, I)}
$$
&lt;p&gt;
我们直接带入上式。
&lt;/p&gt;
$$
x_{t} = \sqrt{1-\beta_{t}}(\sqrt{1-\beta_{t-1}}x_{t-2}+\sqrt{\beta_{t-1}}z_{t-1})+\sqrt{\beta_t}z_t
\\ = \sqrt{(1-\beta_{t})(1-\beta_{t-1})}x_{t-2}+\sqrt{(1-\beta_{t})\beta_{t-1}}z_{t-1}+\sqrt{\beta_t}z_t
$$
&lt;p&gt;
为了表达的更简便一些，我们设$\alpha_t = 1 - \beta_t$,那么上式子可以写成;
&lt;/p&gt;
$$
x_t = \sqrt{\alpha_{t}\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t\beta_{t-1}}z_{t-1}+\sqrt{\beta_t}z_t
$$
&lt;p&gt;
迭代展开：
&lt;/p&gt;
$$
x_t = \sqrt{\alpha_1\alpha_2\alpha_3...\alpha_t}x_0+\sqrt{\beta_t}z_t+\sqrt{\beta_{t-1}\alpha_t}z_{t-1}+\sqrt{\beta_{t-2}\alpha_{t-1}\alpha_{t}}z_{t-2}+...+\sqrt{\beta_1\alpha_2\alpha_3...\alpha_t}z_1
$$
&lt;p&gt;
写的更抽象一点：
&lt;/p&gt;
$$
x_t = \sqrt{\prod_{i = 1}^{t}\alpha_i}x_0+\sum_{i = 1}^{t-1}{\sqrt{(\beta_i\prod_{j=i+1}^{t}\alpha_j)}z_{i}}+\sqrt{\beta_t}z_t
$$
&lt;p&gt;
进一步简化公式，我们设$\overline\alpha=\prod_{i = 1}^{t}\alpha_i,\overline\beta = 1-\overline\alpha$。由于$z_1-z_t$都满足标准正态分布，我们可以直接将式子视为:
&lt;/p&gt;
$$
x_t = \sqrt{\overline\alpha_t}x_0+\sqrt{\overline\beta_t}\overline{z}_t, \ \ \overline{z}_t\sim{N(0, I)}
$$
&lt;p&gt;
或者写成概率分布的形式:
&lt;/p&gt;
$$
q(x_t|x_0) = N(x_t;\sqrt{\overline\alpha_t}x_0,(1-\overline\alpha_t)I)
$$
&lt;p&gt;
即，我们可以将其视为，$x_t$是直接由$x_0$一步加噪而来&lt;/p&gt;
&lt;h3 id=&#34;代码部分&#34;&gt;代码部分&lt;/h3&gt;
&lt;p&gt;pass&lt;/p&gt;
&lt;!-- 我们将其写成概率分布的模式:
$$
q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)
$$
同理，$q(x_{t-1}|x_{t-2})$也可以写成这个形式:
$$
q(x_{t-1}|x_{t-2})=N(x_{t-1};\sqrt{1-\beta_t}x_{t-2},\beta_tI)
$$
迭代展开：
$$
q(x_1,x_2...|x_0)=q
$$ --&gt;
&lt;h2 id=&#34;反向过程&#34;&gt;反向过程&lt;/h2&gt;
&lt;h3 id=&#34;数学推导-1&#34;&gt;数学推导&lt;/h3&gt;
&lt;p&gt;首先我们需要明确反向过程究竟在做什么。反向过程中，我们需要从一张随机生成的噪声图像中逐渐恢复出一张完好的图像。在生成过程中，我们为数不多拥有的，就是随机生成的图像$x_T$和由网络预测而来的噪声$\overline z_t$，和生成时每一个时刻对应的$\alpha$与$\overline\alpha$。而我们最终目标就是将$q(x_{t-1}|x_t)$写成仅与这几者相关的形式。我们从最基本的后验概率模型出发：
&lt;/p&gt;
$$
q(x_{t-1}|x_t) = \frac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}
$$
&lt;p&gt;
这个形式并不是我们想要的，我们想要右边几项更细节的表达。实际上，右边的这几项在前向的过程中都已经被计算过了。我们只需要将其写成有关$x_0$的形式：
&lt;/p&gt;
$$
q(x_t|x_{t-1}, x_0) = \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_t \sim{N(x_{t};\sqrt{\alpha_t}x_{t-1},(1-\alpha)I)}\\ 
q(x_t|x_0) = \sqrt{\overline\alpha_t}x_{0}+\sqrt{1-\overline\alpha_t}\overline{z}_t \sim{N(x_t;\sqrt{\overline\alpha_t}x_0,(1-\overline\alpha_t)I)}\\ 
q(x_{t-1}|x_0) = \sqrt{\overline\alpha_{t-1}}x_{0}+\sqrt{1-\overline\alpha_{t-1}}\overline{z}_{t-1} \sim{N(x_{t-1};\sqrt{\overline\alpha_{t-1}t}x_0,(1-\overline\alpha_{t-1})I)}
$$
&lt;p&gt;
而我们又知道高斯分布的公式:
&lt;/p&gt;
$$
q(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2} \sim N(\mu,\sigma^2)
$$
&lt;p&gt;
我们将上述3式写成标准高斯分布，并带回表达式化简(这里略去数学证明)，可以解得:
&lt;/p&gt;
$$
q(x_{t-1}|x_t,x_0)=\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline\alpha_t}}\overline{z}_t)+\sqrt{\frac{1-\overline{\alpha}_{t-1}}{1-\overline\alpha_t}(1-\alpha_t)} \\ \sim N(x_{t-1};\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline\alpha_t}}\overline{z}_t),\frac{1-\overline{\alpha}_{t-1}}{1-\overline\alpha_t}(1-\alpha_t)I)
$$
&lt;p&gt;
通过上式，我们就可以用为数不多拥有的信息求出$x_{t-1}$,不断重复这个步骤，我们就能推断出$x_0$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>My-first-post</title>
        <link>https://radiance0718.github.io/p/my-first-post/</link>
        <pubDate>Sun, 10 Mar 2024 00:33:03 +0800</pubDate>
        
        <guid>https://radiance0718.github.io/p/my-first-post/</guid>
        <description>&lt;h2 id=&#34;写在最开始&#34;&gt;写在最开始&lt;/h2&gt;
&lt;p&gt;写下这段话的时候，博主是带三狗一枚。&lt;/p&gt;
&lt;p&gt;看着玩的比较好的哥们在科研上做出成就，再看着自己一事无成的3年，于是告诫自己‘博主啊博主，你不能这么堕落下去了’&lt;del&gt;次日：‘在steam上游玩了23h’&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;于是&lt;del&gt;迫于生活的压力&lt;/del&gt;重新开始了博客的更新&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
